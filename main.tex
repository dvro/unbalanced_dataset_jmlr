\documentclass[twoside,11pt]{article}

\usepackage{jmlr2e}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}
\jmlrheading{7}{2016}{1-5}{7/16}{-}{Guillaume Lema\^itre, Fernando Nogueira, and Dayvid V. R. Oliveira}

% Short headings should be running head and authors last names
\ShortHeadings{A python toolbox to tackle the curse of imbalanced datasets in machine learning}{Lema\^itre, Nogueira, and Victor}
\firstpageno{1}

\begin{document}

\title{A python toolbox to tackle the curse of imbalanced datasets in machine learning}
\author{Guillaume Lema\^itre \email g.lemaitre58@gmail.com \\ 
    \addr{LE2I UMR6306, CNRS, Arts et M\'etiers, Universit\'e Bourgogne Franche-Comt\'e} \\ 
    \addr{12 rue de la Fonderie, 71200 Le Creusot, France} \\ 
    \addr{ViCOROB, Universitat de Girona} \\ 
    \addr{Campus Montilivi, Edifici P4, 17071 Girona, Spain}
        \AND
        Fernando Nogueira \email fmfnogueira@gmail.com \\ 
        \addr{theScore, Inc.} \\ 
        \addr{500 King Street West 4\textsuperscript{th} Floor Toronto, Ontario M5V1L9 Canada}
        \AND
        Dayvid V. R. Oliveira \email dvro@cin.ufpe.br \\ 
        \addr{VIISAR Research Group, Centro de Inform\'atica - Universidade Federal de Pernambuco} \\ 
        %\addr{Av. Prof. Luiz Freire, s/n - Cidade Universitária Recife - PE, 50740-540, Brazil}} 
        \addr{Av. Jornalista Aníbal Fernandes, s/n - Cidade Universitária - PE, 50740-560, Brazil}} 
\editor{-}

\maketitle

\begin{abstract}
\emph{UnbalancedDataset} is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition.
The state-of-the-art methods implemented can be categorized into 4 different sampling strategies: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods.
The proposed toolbox only depends of \emph{numpy}, \emph{scipy}, and \emph{scikit-learn} and is distributed under MIT license.
The implementation pattern is identical to the \emph{scikit-learn} API to offer compatibility.
Documentation, unit tests as well as integration tests are provided to allow new developers to contribute.
The toolbox is publicly available in GitHub \url{https://github.com/fmfn/UnbalancedDataset} as well as the documentation \url{http://fmfn.github.io/UnbalancedDataset/}.
\end{abstract}

\begin{keywords}
Imbalanced Dataset, Over-Sampling, Under-Sampling, Ensemble Learning, Machine Learning, Python.
\end{keywords}

\section{Introduction}

Many real world datasets have many samples of some classes (majority classes),
and only a few samples the other class (minority classes). This imbalance gives
rise to the ``class imbalance'' problem~\citep{prati2009data} (or ``curse of imbalanced datasets'')
which is the problem of learning a concept from the class that has a small number of samples compared
to the other classes. 

The class imbalance problem has been encountered in multiple areas such as 
telecommunication managements, bioinformatics, fraud detection, and medical diagnosis,
and has been considered one of the top 10 problems in data mining and 
pattern recognition~\citep{rastgoo2016tackling,yang200610}. 
Imbalanced data substantially compromises the learning process, since most of the 
standard machine learning algorithms expect balanced class distribution or an 
equal misclassification cost~\citep{he2009learning}. For this reason, several
approaches have been specifically proposed to handle such datasets.

Such standalone methods have been implemented mainly in R language~\citep{torgo2010data,kuhn2015caret,dal2013racing}.
Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available~\citep{pedregosa2011scikit,sonnenburg2010shogun}.

In this paper, we present the \textbf{UnbalancedDataset} API, 
\textit{a python toolbox to tackle the curse of imbalanced datasets
in machine learning}. The following sections present the implemented
methods, implemented design, sustainability and continuous integration details,
and finally, the conclusion of this paper, including future functionalities
for the UnbalancedDataset API.

%This problem is frequently referred as ``class imbalance'' problem~\citep{prati2009data} and has been encountered in multiple areas such as telecommunication managements, bioinformatics, fraud detection, and medical diagnosis. 
%Imbalanced data substantially compromises the learning process since most of the standard machine learning algorithms expect balanced class distribution or an equal mis-classification cost~\citep{he2009learning}.
%Medical data are prone to such drawbacks due to the fact that the portion of diseased samples or patients is far lower than healthy cases.
%Furthermore, the detection and classification of minority malignant cases are highly essential so that the sensitivity of developed algorithms need to be maximized.
%Consequently, the problem of imbalanced data is usually addressed by employing different techniques which do not impair the topology of the data.

\section{Project management}

\noindent\textit{Quality insurance} In order to ensure code quality, a set of unit tests is provided leading to a coverage of 99 \% for the release 0.1 of the toolbox.
Furthermore, the code consistency is ensured by following \texttt{PEP8} standards and each new contribution is automatically check through landscape, which provides metrics related to code quality.

\noindent\textit{Continuous integration} To allow user and developer to either use or contribute this toolbox, Travis CI is used to easily integrate new code and ensure back-compatibility.

\noindent\textit{Community-based development} All the development is performed in a collaborative manner.
Tools as git, GitHub, and gitter are used to ease collaborative programming, issue tracking, code integration, and ideas discussions.

\noindent\textit{Documentation} A consistent API documentation is provided using \emph{sphinx} and \emph{numpydoc}.
Additional installation guide, examples, and tutorial are also provided and centralized on GitHub\footnote{\url{http://fmfn.github.io/UnbalancedDataset/}}.

\section{Implementation design}

The implementation rely on \emph{numpy}, \emph{scipy}, and \emph{scikit-learn}.
Each class implements 3 main functions inspired from the \emph{scikit-learn} API:
(i) \emph{fit} computes the parameter values which are later needed to transform the data into a balanced set;
(ii) \emph{transform} performs the sampling and return the data with the desired balancing ratio;
and (iii) \emph{fit\_transform} is equivalent of calling the function \emph{fit} follow the function \emph{transform}.

\section{Implemented methods}

The \textbf{UnbalancedDataset} toolbox provides four different strategies to tackle the problem of imbalanced dataset:
(i) under-sampling, (ii) over-sampling, (iii) a combination of both, and (iv) ensemble learning.
The following sections give an overview of the techniques implemented.

\subsection{Notation and background}

Let $\chi$ an imbalanced dataset with $\chi_{min}$ and $\chi_{maj}$ being the subset of samples belonging to the minority and majority class, respectively.
The balancing ratio of the dataset $\chi$ is defined as:

\begin{equation}
r_{\chi} = \frac{|\chi_{maj}|}{|\chi_{min}|} \ ,
\end{equation}

\noindent where $|\cdot|$ denotes the cardinality of a set.

The balancing process is equivalent to resample $\chi$ into a new dataset $\chi_{res}$ such that $r_{\chi} < r_{\chi_{res}}$.

\subsection{Under-sampling}

Under-sampling refers to the process of reducing the number of samples in $\chi_{maj}$ to obtain the appropriate balancing ratio $r_{\chi_{res}}$.
The following methods are considered to perform such balancing.

\textbf{Random under-sampling} is performed by randomly selecting without replacement a subset of samples from $\chi_{maj}$ to obtain the desired balancing ratio $r_{\chi_{res}}$.

\textbf{Cluster centroids method} refers to the use of a $k$-means to cluster the feature space.
$k$ corresponds to the number of samples in $\chi_{res_{maj}}$ defined by the desired balancing ratio $r_{\chi_{res}}$.
Note that the samples in $\chi_{res_{maj}}$ do not correspond to the original samples of $\chi_{maj}$ and are synthetically generated.

\textbf{Condensed nearest neighbours} Something here.

\textbf{Edited nearest neighbours} Something here.

\textbf{Instance hardness threshold}

\textbf{NearMiss} offers three different methods to under-sample the majority class~\citep{mani2003knn}.
In NearMiss-1, samples $\chi_{maj}$ are selected such that for each sample, the average distance to the $k$ nearest neighbour samples from $\chi_{min}$ is minimum.
NearMiss-2 diverges from NearMiss-1 by considering the $k$ farthest neighbours samples from $\chi_{min}$.
In NearMiss-3, a subset $M$ containing samples from the $\chi_{maj}$ is generated by finding the $m$ nearest neighbours from each sample of $\chi_{min}$.
Then, samples from the subset $M$ are selected such that for each sample, the average distance to the $k$ nearest neighbour samples from $\chi_{min}$ is maximum.

\textbf{One-sided selection} Something here.

\textbf{Neighbourhood cleaning rule} consists of applying two rules depending on the class of each sample~\citep{laurikkala2001improving}.
Let define $x_i$ as a sample of the dataset with its associated class label $y_i$.
Let define $y_m$ as the class of the majority vote of the $k$ nearest neighbours of the sample $x_i$.
If $y_i$ corresponds to $\chi_{maj}$ and $y_i \neq y_m$, $x_i$ is rejected from the final subset.
If $y_i$ corresponds to $\chi_{min}$ class and and $y_i \neq y_m$, then the $k$ nearest neighbours are rejected from the final subset.

\textbf{Tomek links} can be used to under-sample $\chi_{maj}$~\citep{tomek1976two}.
Let define a pair of nearest neighbour samples $(x_i, x_j)$ such that their associated class label $y_i \neq y_j$.
The pair $(x_i, x_j)$ is defined as a Tomek link if, by relaxing the class label differentiation constraint; there is no other sample $x_k$ defined as the nearest neighbour of either $x_i$ or $x_j$.
Under-sampling is performed by removing the samples belonging to $\chi_{maj}$ and forming a Tomek link.

\subsection{Over-sampling}

In the contrary, the data balancing can be performed by over-sampling in which the new samples belonging to the minority class are generated aiming at equalizing the number of samples in both classes.
Two different methods are considered.

\textbf{Random over-sampling} is performed by randomly replicating the samples of the minority class such that the number of samples is equal in both minority and majority classes.

\textbf{SMOTE} is a method to generate synthetic samples in the feature space~\citep{chawla2002smote}.
Let define $x_i$ as a sample belonging to the minority class.
Let define $x_{nn}$ as a randomly selected sample from the $k$ nearest neighbours of $x_i$, with $k$ set to 3.
Therefore, a new sample $x_j$ is generated such that $x_j = x_i + \sigma \left( x_{nn} - x_i \right)$, where $\sigma$ is a random number in the interval $\left[0,1\right]$.
Three other variants of this algorithm exist: (i) SMOTE borderline 1, (ii) SMOTE borderline 2, and (iii) SMOTE SVM.

\subsection{Combination of over- and under-sampling}

Subsequently, over-sampling methods can be combined with under-sampling methods to clean the subset created.
In that regard, two different combinations are tested.

\textbf{SMOTE + Tomek links} are combined to clean the samples created using SMOTE~\citep{batista2003balancing}.
SMOTE over-sampling can lead to over-fitting which can be avoided by removing the Tomek links from both majority and minority classes~\citep{prati2009data}.
\textbf{SMOTE + edited nearest neighbours} are combined for the same aforementioned reason~\citep{batista2004study}.

\subsection{Ensemble learning}

Under-sampling methods implies that samples of the majority class will be lost during the balancing procedure.
Ensemble methods can offer an alternative to use most of the samples.
In fact, an ensemble of balanced set will be created and used to later train any classifier.
Two methods are available to build such ensemble.

\textbf{Balance cascade} Something here

\textbf{Easy ensemble} Something here

% \begin{figure}
%   \centering
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:clustercentroids}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_cluster_centroids_001.png}} \hfill
%   \subfigure[]{\label{subfig:condensednearestneighbour}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_condensed_nearest_neighbour_001.png}} \hfill
%   \subfigure[]{\label{subfig:editedneasrestneighbours}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_edited_nearest_neighbours_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:nearmiss1}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_nearmiss_1_001.png}} \hfill
%   \subfigure[]{\label{subfig:nearmiss2}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_nearmiss_2_001.png}} \hfill
%   \subfigure[]{\label{subfig:nearmiss3}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_nearmiss_3_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:onesidedselection}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_one_sided_selection_001.png}} \hfill
%   \subfigure[]{\label{subfig:neighbourhoodcleaningrule}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_neighbourhood_cleaning_rule_001.png}} \hfill
%   \subfigure[]{\label{subfig:tomeklinks}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_tomek_links_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:smote}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_001.png}} \hfill
%   \subfigure[]{\label{subfig:smoteborderline1}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_bordeline_1_001.png}} \hfill
%   \subfigure[]{\label{subfig:smoteborderline2}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_bordeline_2_001.png}}
%   \hspace*{\fill}\\
%   \hspace*{\fill}
%   \subfigure[]{\label{subfig:smotesvm}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_svm_001.png}} \hfill
%   \subfigure[]{\label{subfig:smoteenn}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_enn_001.png}} \hfill
%   \subfigure[]{\label{subfig:smotetomek}\includegraphics[width=0.3\linewidth]{sphx_glr_plot_smote_tomek_001.png}}
%   \hspace*{\fill}
%   \caption{Toy examples for the different sampling methods.}
%   \label{fig:methods}
% \end{figure}

\section{Conclusion}

\acks{We would like to acknowledge support for this project
from git, GitHub, Travis CI, and Gitter.}

\bibliography{bibtex}


\end{document}
